{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook was auto-generated by GitHub Copilot Chat and is meant for initial setup only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Prompt Engineering\n",
    "Prompt engineering is the process of designing and optimizing prompts for natural language processing tasks. It involves selecting the right prompts, tuning their parameters, and evaluating their performance. Prompt engineering is crucial for achieving high accuracy and efficiency in NLP models. In this section, we will explore the basics of prompt engineering using the OpenAI models for exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Tokenization\n",
    "Explore Tokenization using tiktoken, an open-source fast tokenizer from OpenAI\n",
    "See [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb?WT.mc_id=academic-105485-koreyst) for more examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198, 41, 20089, 374, 279, 18172, 11841, 505, 279, 8219, 323, 279, 7928, 304, 279, 25450, 744, 13, 1102, 374, 264, 6962, 14880, 449, 264, 3148, 832, 7716, 52949, 339, 430, 315, 279, 8219, 11, 719, 1403, 9976, 7561, 34902, 3115, 430, 315, 682, 279, 1023, 33975, 304, 279, 25450, 744, 11093, 13, 50789, 374, 832, 315, 279, 72021, 6302, 9621, 311, 279, 19557, 8071, 304, 279, 3814, 13180, 11, 323, 706, 1027, 3967, 311, 14154, 86569, 2533, 1603, 12715, 3925, 13, 1102, 374, 7086, 1306, 279, 13041, 10087, 50789, 8032, 777, 60, 3277, 19894, 505, 9420, 11, 50789, 649, 387, 10107, 3403, 369, 1202, 27000, 3177, 311, 6445, 9621, 35612, 17706, 508, 60, 323, 374, 389, 5578, 279, 4948, 1481, 1315, 478, 5933, 1665, 304, 279, 3814, 13180, 1306, 279, 17781, 323, 50076, 627]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[b'\\n',\n",
       " b'J',\n",
       " b'upiter',\n",
       " b' is',\n",
       " b' the',\n",
       " b' fifth',\n",
       " b' planet',\n",
       " b' from',\n",
       " b' the',\n",
       " b' Sun',\n",
       " b' and',\n",
       " b' the',\n",
       " b' largest',\n",
       " b' in',\n",
       " b' the',\n",
       " b' Solar',\n",
       " b' System',\n",
       " b'.',\n",
       " b' It',\n",
       " b' is',\n",
       " b' a',\n",
       " b' gas',\n",
       " b' giant',\n",
       " b' with',\n",
       " b' a',\n",
       " b' mass',\n",
       " b' one',\n",
       " b'-th',\n",
       " b'ousand',\n",
       " b'th',\n",
       " b' that',\n",
       " b' of',\n",
       " b' the',\n",
       " b' Sun',\n",
       " b',',\n",
       " b' but',\n",
       " b' two',\n",
       " b'-and',\n",
       " b'-a',\n",
       " b'-half',\n",
       " b' times',\n",
       " b' that',\n",
       " b' of',\n",
       " b' all',\n",
       " b' the',\n",
       " b' other',\n",
       " b' planets',\n",
       " b' in',\n",
       " b' the',\n",
       " b' Solar',\n",
       " b' System',\n",
       " b' combined',\n",
       " b'.',\n",
       " b' Jupiter',\n",
       " b' is',\n",
       " b' one',\n",
       " b' of',\n",
       " b' the',\n",
       " b' brightest',\n",
       " b' objects',\n",
       " b' visible',\n",
       " b' to',\n",
       " b' the',\n",
       " b' naked',\n",
       " b' eye',\n",
       " b' in',\n",
       " b' the',\n",
       " b' night',\n",
       " b' sky',\n",
       " b',',\n",
       " b' and',\n",
       " b' has',\n",
       " b' been',\n",
       " b' known',\n",
       " b' to',\n",
       " b' ancient',\n",
       " b' civilizations',\n",
       " b' since',\n",
       " b' before',\n",
       " b' recorded',\n",
       " b' history',\n",
       " b'.',\n",
       " b' It',\n",
       " b' is',\n",
       " b' named',\n",
       " b' after',\n",
       " b' the',\n",
       " b' Roman',\n",
       " b' god',\n",
       " b' Jupiter',\n",
       " b'.[',\n",
       " b'19',\n",
       " b']',\n",
       " b' When',\n",
       " b' viewed',\n",
       " b' from',\n",
       " b' Earth',\n",
       " b',',\n",
       " b' Jupiter',\n",
       " b' can',\n",
       " b' be',\n",
       " b' bright',\n",
       " b' enough',\n",
       " b' for',\n",
       " b' its',\n",
       " b' reflected',\n",
       " b' light',\n",
       " b' to',\n",
       " b' cast',\n",
       " b' visible',\n",
       " b' shadows',\n",
       " b',[',\n",
       " b'20',\n",
       " b']',\n",
       " b' and',\n",
       " b' is',\n",
       " b' on',\n",
       " b' average',\n",
       " b' the',\n",
       " b' third',\n",
       " b'-b',\n",
       " b'right',\n",
       " b'est',\n",
       " b' natural',\n",
       " b' object',\n",
       " b' in',\n",
       " b' the',\n",
       " b' night',\n",
       " b' sky',\n",
       " b' after',\n",
       " b' the',\n",
       " b' Moon',\n",
       " b' and',\n",
       " b' Venus',\n",
       " b'.\\n']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXERCISE:\n",
    "# 1. Run the exercise as is first\n",
    "# 2. Change the text to any prompt input you want to use & re-run to see tokens\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Define the prompt you want tokenized\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "# Set the model you want encoding for\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Encode the text - gives you the tokens in integer form\n",
    "tokens = encoding.encode(text)\n",
    "print(tokens);\n",
    "\n",
    "# Decode the integers to see what the text versions look like\n",
    "[encoding.decode_single_token_bytes(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Validate OpenAI API Key Setup\n",
    "\n",
    "Run the code below to verify that your OpenAI endpoint is set up correctly. The code just tries a simple basic prompt and validates the completion. Input `oh say can you see` should complete along the lines of `by the dawn's early light..`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Environment Variables and Current Values:\n",
      "============================================================\n",
      "AZURE_OPENAI_ENDPOINT: https://mwool-mj66edpx-eastus2.openai.azure.com/openai/v1\n",
      "AZURE_OPENAI_DEPLOYMENT: gpt-5.2-chat\n",
      "AZURE_OPENAI_API_KEY: ***qbvK\n",
      "API_VERSION: 2024-02-01\n",
      "============================================================\n",
      "\n",
      "Request failed: Error code: 400 - {'error': {'message': \"Unsupported value: 'temperature' does not support 0 with this model. Only the default (1) value is supported.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}\n",
      "Check that AZURE_OPENAI_ENDPOINT is correct and AZURE_OPENAI_DEPLOYMENT matches your deployed model name.\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Unsupported value: 'temperature' does not support 0 with this model. Only the default (1) value is supported.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     81\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[33m```\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m```\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m## 3. Run the prompt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m response = \u001b[43mget_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mget_completion\u001b[39m\u001b[34m(prompt, temperature)\u001b[39m\n\u001b[32m     47\u001b[39m messages = [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}]\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m   \u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeployment_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                                         \u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# If a 404 occurs, retry once with the '/openai' suffix if the original endpoint didn't include it\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Unsupported value: 'temperature' does not support 0 with this model. Only the default (1) value is supported.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Print expected environment variables and their actual values\n",
    "print(\"Expected Environment Variables and Current Values:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "endpoint_env = os.environ.get('AZURE_OPENAI_ENDPOINT', 'NOT SET')\n",
    "deployment_name = os.environ.get('AZURE_OPENAI_DEPLOYMENT', 'NOT SET')\n",
    "api_key = os.environ.get('AZURE_OPENAI_API_KEY', 'NOT SET')\n",
    "api_version = os.environ.get('AZURE_OPENAI_API_VERSION', os.environ.get('OPENAI_API_VERSION', '2024-02-01'))\n",
    "\n",
    "print(f\"AZURE_OPENAI_ENDPOINT: {endpoint_env}\")\n",
    "print(f\"AZURE_OPENAI_DEPLOYMENT: {deployment_name}\")\n",
    "print(f\"AZURE_OPENAI_API_KEY: {'***' + api_key[-4:] if api_key != 'NOT SET' and len(api_key) > 4 else api_key}\")\n",
    "print(f\"API_VERSION: {api_version}\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Validate required variables are set\n",
    "if endpoint_env == 'NOT SET' or deployment_name == 'NOT SET' or api_key == 'NOT SET':\n",
    "    raise ValueError(\"Missing required environment variables. Please check your .env file.\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=endpoint_env,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "prompt = \"Oh say can you see\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Response: {completion.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Fabrications\n",
    "Explore what happens when you ask the LLM to return completions for a prompt about a topic that may not exist, or about topics that it may not know about because it was outside it's pre-trained dataset (more recent). See how the response changes if you try a different prompt, or a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Set the text for simple prompt or primary content\n",
    "## Prompt shows a template format with text in it - add cues, commands etc if needed\n",
    "## Run the completion \n",
    "text = f\"\"\"\n",
    "generate a lesson plan on the Martian War of 2076.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Instruction Based \n",
    "Use the \"text\" variable to set the primary content \n",
    "and the \"prompt\" variable to provide an instruction related to that primary content.\n",
    "\n",
    "Here we ask the model to summarize the text for a second-grade student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Example\n",
    "# https://platform.openai.com/playground/p/default-summarize\n",
    "\n",
    "## Example text\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "Summarize content you are provided with for a second-grade student.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Complex Prompt \n",
    "Try a request that has system, user and assistant messages \n",
    "System sets assistant context\n",
    "User & Assistant messages provide multi-turn conversation context\n",
    "\n",
    "Note how the assistant personality is set to \"sarcastic\" in the system context. \n",
    "Try using a different personality context. Or try a different series of input/output messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a sarcastic assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who do you think won? The Los Angeles Dodgers of course.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore Your Intuition\n",
    "The above examples give you patterns that you can use to create new prompts (simple, complex, instruction etc.) - try creating other exercises to explore some of the other ideas we've talked about like examples, cues and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
